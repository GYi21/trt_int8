{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/opt/projects/aoi/led_detection/training/scratch/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_380511/2875996582.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Detected DataParallel, unwrapping the model...\n",
      "‚úÖ Real image input shape: torch.Size([1, 3, 640, 640])\n",
      "‚úÖ ONNXWrapper Forward running, input shape: torch.Size([1, 3, 640, 640])\n",
      "‚úÖ ONNXWrapper Output: scores torch.Size([1, 18]), labels torch.Size([1, 18]), boxes torch.Size([1, 18, 4])\n",
      "üì¶ PyTorch Predicted boxes: tensor([[[ 39.9766, 447.2587,  81.3504, 480.3082],\n",
      "         [ 52.9102, 221.9566,  92.4365, 256.0235],\n",
      "         [439.2004, 467.7859, 481.3566, 503.3602],\n",
      "         [172.5506, 452.7335, 214.7695, 488.5393],\n",
      "         [508.3899, 471.9164, 550.7650, 507.0966],\n",
      "         [183.8363, 228.0184, 227.6414, 263.4944],\n",
      "         [118.2582, 223.6453, 162.2453, 259.8027],\n",
      "         [105.3479, 449.3438, 148.9051, 485.3590],\n",
      "         [456.8612, 244.0427, 493.2340, 276.1395],\n",
      "         [252.0436, 231.6617, 293.3106, 266.3990],\n",
      "         [305.9417, 460.1208, 347.8703, 495.4235],\n",
      "         [374.4185, 464.6841, 413.6945, 498.9632],\n",
      "         [321.7256, 236.2244, 358.1698, 268.5798],\n",
      "         [523.9587, 247.5427, 560.0939, 279.7383],\n",
      "         [238.2290, 455.8893, 282.4760, 492.0183],\n",
      "         [388.9859, 239.8381, 426.7308, 272.3636],\n",
      "         [591.9474, 251.6528, 627.8399, 283.5241],\n",
      "         [578.0944, 476.3805, 613.2239, 509.3814]]], device='cuda:0',\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "‚úÖ ONNXWrapper Forward running, input shape: torch.Size([1, 3, 640, 640])\n",
      "‚úÖ ONNXWrapper Output: scores torch.Size([1, 18]), labels torch.Size([1, 18]), boxes torch.Size([1, 18, 4])\n",
      "üì¶ PyTorch Predicted boxes: tensor([[[ 39.9766, 447.2587,  81.3504, 480.3082],\n",
      "         [ 52.9102, 221.9566,  92.4365, 256.0235],\n",
      "         [439.2004, 467.7859, 481.3566, 503.3602],\n",
      "         [172.5506, 452.7335, 214.7695, 488.5393],\n",
      "         [508.3899, 471.9164, 550.7650, 507.0966],\n",
      "         [183.8363, 228.0184, 227.6414, 263.4944],\n",
      "         [118.2582, 223.6453, 162.2453, 259.8027],\n",
      "         [105.3479, 449.3438, 148.9051, 485.3590],\n",
      "         [456.8612, 244.0427, 493.2340, 276.1395],\n",
      "         [252.0436, 231.6617, 293.3106, 266.3990],\n",
      "         [305.9417, 460.1208, 347.8703, 495.4235],\n",
      "         [374.4185, 464.6841, 413.6945, 498.9632],\n",
      "         [321.7256, 236.2244, 358.1698, 268.5798],\n",
      "         [523.9587, 247.5427, 560.0939, 279.7383],\n",
      "         [238.2290, 455.8893, 282.4760, 492.0183],\n",
      "         [388.9859, 239.8381, 426.7308, 272.3636],\n",
      "         [591.9474, 251.6528, 627.8399, 283.5241],\n",
      "         [578.0944, 476.3805, 613.2239, 509.3814]]], device='cuda:0',\n",
      "       grad_fn=<UnsqueezeBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guoy/led_detection/training/RetinaNet/retinanet/anchors.py:24: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  image_shape = np.array(image_shape)\n",
      "/home/guoy/led_detection/training/RetinaNet/retinanet/anchors.py:38: TracerWarning: torch.from_numpy results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  return torch.from_numpy(all_anchors.astype(np.float32)).cuda()\n",
      "/home/guoy/led_detection/training/RetinaNet/retinanet/utils.py:138: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  height = torch.tensor(img.shape[2], dtype=boxes.dtype, device=device)  # Á°Æ‰øù width Âíå height Âú® boxes.device ‰∏ä\n",
      "/home/guoy/led_detection/training/RetinaNet/retinanet/utils.py:138: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  height = torch.tensor(img.shape[2], dtype=boxes.dtype, device=device)  # Á°Æ‰øù width Âíå height Âú® boxes.device ‰∏ä\n",
      "/home/guoy/led_detection/training/RetinaNet/retinanet/utils.py:139: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  width = torch.tensor(img.shape[3], dtype=boxes.dtype, device=device)\n",
      "/home/guoy/led_detection/training/RetinaNet/retinanet/utils.py:139: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  width = torch.tensor(img.shape[3], dtype=boxes.dtype, device=device)\n",
      "/home/guoy/led_detection/training/RetinaNet/retinanet/model.py:268: TracerWarning: torch.Tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  finalScores = torch.Tensor([]).cuda() if torch.cuda.is_available() else torch.Tensor([])\n",
      "/home/guoy/led_detection/training/RetinaNet/retinanet/model.py:269: TracerWarning: torch.Tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  finalAnchorBoxesIndexes = torch.Tensor([]).long().cuda() if torch.cuda.is_available() else torch.Tensor([]).long()\n",
      "/home/guoy/led_detection/training/RetinaNet/retinanet/model.py:270: TracerWarning: torch.Tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  finalAnchorBoxesCoordinates = torch.Tensor([]).cuda() if torch.cuda.is_available() else torch.Tensor([])\n",
      "/home/guoy/led_detection/training/RetinaNet/retinanet/model.py:275: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if scores_over_thresh.sum() == 0:\n",
      "/home/guoy/led_detection/training/RetinaNet/retinanet/model.py:283: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
      "  finalResult[0].extend(scores[anchors_nms_idx])\n",
      "/home/guoy/led_detection/training/RetinaNet/retinanet/model.py:283: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
      "  finalResult[0].extend(scores[anchors_nms_idx])\n",
      "/home/guoy/led_detection/training/RetinaNet/retinanet/model.py:284: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  finalResult[1].extend(torch.tensor([i] * anchors_nms_idx.shape[0]))\n",
      "/home/guoy/led_detection/training/RetinaNet/retinanet/model.py:284: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
      "  finalResult[1].extend(torch.tensor([i] * anchors_nms_idx.shape[0]))\n",
      "/home/guoy/led_detection/training/RetinaNet/retinanet/model.py:284: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
      "  finalResult[1].extend(torch.tensor([i] * anchors_nms_idx.shape[0]))\n",
      "/home/guoy/led_detection/training/RetinaNet/retinanet/model.py:285: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
      "  finalResult[2].extend(anchorBoxes[anchors_nms_idx])\n",
      "/home/guoy/led_detection/training/RetinaNet/retinanet/model.py:285: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
      "  finalResult[2].extend(anchorBoxes[anchors_nms_idx])\n",
      "/home/guoy/led_detection/training/RetinaNet/retinanet/model.py:288: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  finalAnchorBoxesIndexesValue = torch.tensor([i] * anchors_nms_idx.shape[0])\n",
      "/tmp/ipykernel_380511/2875996582.py:41: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if scores.numel() == 0 or labels.numel() == 0 or boxes.numel() == 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully exported ONNX: /home/guoy/led_detection/training/RetinaNet/RetinaNet_v1.onnx\n",
      "‚úÖ ONNX Inference scores.shape=(1, 18)\n",
      "‚úÖ ONNX Inference labels.shape=(1, 18)\n",
      "‚úÖ ONNX Inference boxes.shape=(1, 18, 4)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import onnx\n",
    "\n",
    "# üöÄ **Define Normalizer**\n",
    "class Normalizer:\n",
    "    def __init__(self):\n",
    "        self.mean = np.array([0.485, 0.456, 0.406], dtype=np.float32).reshape(1, 1, 3)\n",
    "        self.std = np.array([0.229, 0.224, 0.225], dtype=np.float32).reshape(1, 1, 3)\n",
    "\n",
    "    def __call__(self, img):\n",
    "        return (img - self.mean) / self.std\n",
    "\n",
    "# **ONNX Wrapper Class**\n",
    "class ONNXWrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(ONNXWrapper, self).__init__()\n",
    "        self.model = model\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"‚úÖ ONNXWrapper Forward running, input shape: {x.shape}\")\n",
    "        scores, labels, boxes = self.model(x)\n",
    "\n",
    "\n",
    "        # **Apply Sigmoid normalization to scores**\n",
    "        scores = self.sigmoid(scores).to(dtype=torch.float32)\n",
    "        labels = labels.to(dtype=torch.float32)  # ‚úÖ ÊîπÊàê int32\n",
    "        boxes = boxes.to(dtype=torch.float32)\n",
    "\n",
    "        \n",
    "        boxes = boxes.unsqueeze(0) if boxes.dim() == 2 else boxes  # (18, 4) ‚Üí (1, 18, 4)\n",
    "        scores = scores.unsqueeze(0)  # (18,) ‚Üí (1, 18)\n",
    "        labels = labels.unsqueeze(0)  # (18,) ‚Üí (1, 18)\n",
    "\n",
    "        # **Prevent empty outputs from affecting ONNX inference**\n",
    "        if scores.numel() == 0 or labels.numel() == 0 or boxes.numel() == 0:\n",
    "            print(\"‚ö†Ô∏è No detections found, filling with default values\")\n",
    "            scores = torch.tensor([[0.01]], dtype=torch.float32, device=x.device)\n",
    "            labels = torch.tensor([[0]], dtype=torch.int64, device=x.device)\n",
    "            boxes = torch.tensor([[[0.0, 0.0, 1.0, 1.0]]], dtype=torch.float32, device=x.device)\n",
    "\n",
    "\n",
    "\n",
    "        \"\"\"        # **Ensure boxes have the correct dimension (batch_size, num_detections, 4)**\n",
    "        if boxes.dim() == 2:\n",
    "            boxes = boxes.unsqueeze(1)  # Add an extra dimension\n",
    "        elif boxes.dim() == 3:\n",
    "            assert boxes.shape[2] == 4, \"Boxes must have 4 coordinate values\"\n",
    "        \n",
    "        # **Adjust box dimensions**\n",
    "        if boxes.shape[1] == 1:\n",
    "            boxes = boxes.squeeze(1)  # Remove unnecessary dimension\"\"\"\n",
    "\n",
    "        print(f\"‚úÖ ONNXWrapper Output: scores {scores.shape}, labels {labels.shape}, boxes {boxes.shape}\")\n",
    "        print(f\"üì¶ PyTorch Predicted boxes: {boxes}\")\n",
    "        return scores, labels, boxes\n",
    "\n",
    "\n",
    "# **Load PyTorch model**\n",
    "model_path = \"/home/guoy/led_detection/training/RetinaNet/RetinaNet_model_final.pt\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = torch.load(model_path, map_location=device)\n",
    "\n",
    "# **Unwrap DataParallel**\n",
    "if isinstance(model, torch.nn.DataParallel):\n",
    "    print(\"‚ö†Ô∏è Detected DataParallel, unwrapping the model...\")\n",
    "    model = model.module\n",
    "\n",
    "# **Wrap the forward() function**\n",
    "wrapped_model = ONNXWrapper(model).to(device)\n",
    "wrapped_model.eval()\n",
    "\n",
    "# **üöÄ Read and normalize a real image**\n",
    "image_path = \"/home/guoy/led_detection/training/RetinaNet/000002_2024_10_28_15_40_10_540_0.jpg\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "img_np = np.array(image).astype(np.float32) / 255.0\n",
    "img_np = Normalizer()(img_np)\n",
    "img_np = np.transpose(img_np, (2, 0, 1))\n",
    "\n",
    "# **Create PyTorch input tensor**\n",
    "input_tensor = torch.tensor(img_np, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "print(f\"‚úÖ Real image input shape: {input_tensor.shape}\")\n",
    "\n",
    "# **üöÄ Test PyTorch forward() first**\n",
    "wrapped_model(input_tensor)\n",
    "\n",
    "# **Export to ONNX**\n",
    "onnx_path = \"/home/guoy/led_detection/training/RetinaNet/RetinaNet_v1.onnx\"\n",
    "torch.onnx.export(\n",
    "    wrapped_model,\n",
    "    input_tensor,\n",
    "    onnx_path,\n",
    "    opset_version=11,\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"scores\", \"labels\", \"boxes\"],\n",
    "    dynamic_axes={\n",
    "        \"scores\": {0: \"batch_size\", 1: \"num_detections\"},\n",
    "        \"labels\": {0: \"batch_size\", 1: \"num_detections\"},\n",
    "        \"boxes\": {0: \"batch_size\", 1: \"num_detections\", 2: \"coords\"}  # coords represent bounding box coordinates\n",
    "    },\n",
    "    do_constant_folding=True \n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Successfully exported ONNX: {onnx_path}\")\n",
    "\n",
    "try:\n",
    "    ort_session = ort.InferenceSession(onnx_path)\n",
    "    \n",
    "    # Get input name\n",
    "    input_name = ort_session.get_inputs()[0].name\n",
    "    \n",
    "    # Ensure the input data has the correct shape\n",
    "    onnx_input = input_tensor.detach().cpu().numpy()\n",
    "    \n",
    "    # Perform ONNX inference\n",
    "    outputs = ort_session.run(None, {input_name: onnx_input})\n",
    "    \n",
    "    # Ensure the output shapes match expectations\n",
    "    print(f\"‚úÖ ONNX Inference scores.shape={outputs[0].shape}\")\n",
    "    print(f\"‚úÖ ONNX Inference labels.shape={outputs[1].shape}\")\n",
    "    print(f\"‚úÖ ONNX Inference boxes.shape={outputs[2].shape}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ONNX Inference failed: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
