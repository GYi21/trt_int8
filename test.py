import tensorrt as trt
import numpy as np
import os
import pycuda.driver as cuda
import pycuda.autoinit
import ctypes
import warnings

warnings.filterwarnings("ignore", category=DeprecationWarning)

onnx_file = "./RetinaNet_v3.onnx"
engine_file = "./model_8.trt"
calibration_data_path = "./cali_datei"

# âœ… TensorRT Logger
logger = trt.Logger(trt.Logger.INFO)
builder = trt.Builder(logger)
network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
parser = trt.OnnxParser(network, logger)

# âœ… Load ONNX Model
with open(onnx_file, "rb") as model:
    if not parser.parse(model.read()):
        for i in range(parser.num_errors):
            print(f"ONNX Parsing Error {i}: {parser.get_error(i)}")
        raise RuntimeError("Failed to parse ONNX model")
    else:
        print("âœ… ONNX model successfully parsed!")

# âœ… Create TensorRT Config
config = builder.create_builder_config()
config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 8 << 30)
config.set_flag(trt.BuilderFlag.INT8)  # ğŸ”¥ Enable INT8
config.set_flag(trt.BuilderFlag.DISABLE_TIMING_CACHE)  # ğŸ”¥ Avoid Timing Cache Errors

import tensorrt as trt
import numpy as np
import os
import pycuda.driver as cuda
import pycuda.autoinit
import ctypes

class Int8Calibrator(trt.IInt8EntropyCalibrator2):
    def __init__(self, calibration_data_folder, batch_size=1, cache_file="calibration.cache"):
        super(Int8Calibrator, self).__init__()

        self.batch_size = batch_size
        self.cache_file = cache_file

        # ğŸ”¥ è·å–æ‰€æœ‰ .npy æ–‡ä»¶
        self.image_paths = sorted([
            os.path.join(calibration_data_folder, f)
            for f in os.listdir(calibration_data_folder) if f.endswith(".npy")
        ])
        self.current_index = 0

        # âœ… é¢„åˆ†é… GPU è®¾å¤‡å†…å­˜ (Device Memory)
        self.device_input = cuda.mem_alloc(batch_size * 3 * 640 * 640 * np.float32().nbytes)

        # âœ… é¢„åˆ†é… CPU (Host) å†…å­˜
        self.pinned_memory = np.zeros((batch_size, 3, 640, 640), dtype=np.float32)

        # âœ… åˆ›å»ºä¸€ä¸ªç”Ÿæˆå™¨ (batches) æä¾›æ•°æ®
        self.batches = self._batch_generator()

        print(f"âœ… Initialized Int8Calibrator with {len(self.image_paths)} calibration images")

    def _batch_generator(self):
        """ ç”Ÿæˆæ ¡å‡†æ•°æ® """
        for i in range(0, len(self.image_paths), self.batch_size):
            batch = []
            for j in range(self.batch_size):
                if i + j < len(self.image_paths):
                    npy_file = self.image_paths[i + j]
                    img = np.load(npy_file).astype(np.float32)

                if img.shape == (1, 3, 640, 640):
                    img = img.unsqueeze()
                
                batch.append(img)

            if len(batch) > 0:
                yield np.array(batch)
            else:
                raise RuntimeError("âŒ Batch data is empty!")

    def get_batch_size(self):
        return self.batch_size

    def get_batch(self, names):
        try:
            # âœ… ä»ç”Ÿæˆå™¨è·å–æ•°æ®
            data = next(self.batches)
            print(f"âœ… Loaded batch, shape: {data.shape}")

            # âœ… å¤åˆ¶æ•°æ®åˆ° GPU
            cuda.memcpy_htod(self.device_input, data)
            print(f"âœ… Copy batch success.")

            # âœ… è¿”å› GPU è®¾å¤‡å†…å­˜æŒ‡é’ˆ
            return [int(self.device_input)]

        except StopIteration:
            print("âŒ No more calibration data available")
            return None

    def read_calibration_cache(self):
        """âœ… è¯»å– INT8 æ ¡å‡†ç¼“å­˜"""
        try:
            with open(self.cache_file, "rb") as f:
                cache = f.read()
            print(f"âœ… Using existing INT8 calibration cache: {self.cache_file}")
            return cache
        except FileNotFoundError:
            print(f"âŒ Calibration cache not found, running fresh calibration")
            return None

    def write_calibration_cache(self, cache):
        """âœ… å†™å…¥ INT8 æ ¡å‡†ç¼“å­˜"""
        if cache is None or len(cache) == 0:
            print("âŒ Calibration cache is empty, possible issue!")
        else:
            with open(self.cache_file, "wb") as f:
                f.write(cache)
            print(f"âœ… Calibration cache saved: {self.cache_file}")

# âœ… Apply INT8 Calibrator
config.int8_calibrator = Int8Calibrator(calibration_data_path)

# âœ… Create Optimization Profile
profile = builder.create_optimization_profile()
profile.set_shape("input", (1, 3, 640, 640), (1, 3, 640, 640), (1, 3, 640, 640))  # ğŸ”¥ Static batch size = 1
config.add_optimization_profile(profile)

# âœ… Build TensorRT INT8 Engine
serialized_engine = builder.build_serialized_network(network, config)

if serialized_engine is None:
    raise RuntimeError("âŒ Failed to build TensorRT INT8 engine!")

# âœ… Save TensorRT INT8 Engine
with open(engine_file, "wb") as f:
    f.write(serialized_engine)

print(f"âœ… INT8 TensorRT engine saved at: {engine_file}")
