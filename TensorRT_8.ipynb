{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the INT8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "calibration_data_path = \"/home/guoy/led_detection/training/yolov8/yolo_val/extra_npy\"\n",
    "files = sorted([os.path.join(calibration_data_path, f) for f in os.listdir(calibration_data_path) if f.endswith(\".npy\")])\n",
    "\n",
    "for f in files[:5]:  # 只检查前5个\n",
    "    data = np.load(f)\n",
    "    print(f\"检查 {f}: shape={data.shape}, dtype={data.dtype}, min={data.min()}, max={data.max()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尝试1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected input shape: [1, 3, 640, 640]\n",
      "[03/20/2025-23:11:58] [TRT] [I] The logger passed into createInferBuilder differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_970254/888793134.py:79: DeprecationWarning: Use Deprecated in TensorRT 10.1. Superseded by explicit quantization. instead.\n",
      "  config.int8_calibrator = int8_calibrator\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03/20/2025-23:12:00] [TRT] [I] Calibration table does not match calibrator algorithm type.\n",
      "[03/20/2025-23:12:00] [TRT] [I] Perform graph optimization on calibration graph.\n",
      "[03/20/2025-23:12:00] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[03/20/2025-23:12:00] [TRT] [I] Compiler backend is used during engine build.\n",
      "[03/20/2025-23:12:01] [TRT] [I] Detected 1 inputs and 3 output network tensors.\n",
      "[03/20/2025-23:12:02] [TRT] [I] Total Host Persistent Memory: 313120 bytes\n",
      "[03/20/2025-23:12:02] [TRT] [I] Total Device Persistent Memory: 673280 bytes\n",
      "[03/20/2025-23:12:02] [TRT] [I] Max Scratch Memory: 2151424 bytes\n",
      "[03/20/2025-23:12:02] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 315 steps to complete.\n",
      "[03/20/2025-23:12:02] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 43.6311ms to assign 34 blocks to 315 nodes requiring 70206976 bytes.\n",
      "[03/20/2025-23:12:02] [TRT] [I] Total Activation Memory: 70206976 bytes\n",
      "[03/20/2025-23:12:02] [TRT] [I] Total Weights Memory: 215006528 bytes\n",
      "[03/20/2025-23:12:02] [TRT] [I] Compiler backend is used during engine execution.\n",
      "[03/20/2025-23:12:02] [TRT] [I] Engine generation completed in 1.97606 seconds.\n",
      "[03/20/2025-23:12:02] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +68, now: CPU 0, GPU 285 (MiB)\n",
      "[03/20/2025-23:12:02] [TRT] [I] Starting Calibration.\n",
      "[03/20/2025-23:12:02] [TRT] [I]   Post Processing Calibration data in 7.1e-07 seconds.\n",
      "[03/20/2025-23:12:02] [TRT] [E] Unexpected exception _Map_base::at\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] Exception caught in get_batch(): Unable to cast Python instance to C++ type (#define PYBIND11_DETAILED_ERROR_MESSAGES or compile in debug mode for details)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to build TensorRT engine",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 84\u001b[0m\n\u001b[1;32m     81\u001b[0m serialized_engine \u001b[38;5;241m=\u001b[39m builder\u001b[38;5;241m.\u001b[39mbuild_serialized_network(network, config)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m serialized_engine \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to build TensorRT engine\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     86\u001b[0m engine_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/guoy/led_detection/training/RetinaNet/RetinaNet_v1_int8.engine\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(engine_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to build TensorRT engine"
     ]
    }
   ],
   "source": [
    "import tensorrt as trt\n",
    "import numpy as np\n",
    "import os\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "import ctypes\n",
    "import onnx\n",
    "\n",
    "class RetinaNetInt8Calibrator(trt.IInt8EntropyCalibrator2):\n",
    "    def __init__(self, calibration_data_folder, onnx_model_path, batch_size=1):\n",
    "        super(RetinaNetInt8Calibrator, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.data_files = [os.path.join(calibration_data_folder, f) \n",
    "                           for f in os.listdir(calibration_data_folder) if f.endswith(\".npy\")]\n",
    "        self.data_files.sort()\n",
    "        self.current_index = 0\n",
    "        self.input_shape = self.get_input_shape(onnx_model_path)\n",
    "        self.host_input = np.empty(self.input_shape, dtype=np.float32)  # 直接用 NumPy 申请 Host 内存\n",
    "    \n",
    "    def get_input_shape(self, onnx_model_path):\n",
    "        onnx_model = onnx.load(onnx_model_path)\n",
    "        input_tensor = onnx_model.graph.input[0]\n",
    "        input_shape = []\n",
    "        for dim in input_tensor.type.tensor_type.shape.dim:\n",
    "            if dim.dim_value is not None and dim.dim_value > 0:\n",
    "                input_shape.append(dim.dim_value)\n",
    "            else:\n",
    "                input_shape.append(self.batch_size)  # 避免 None 值\n",
    "        print(\"Expected input shape:\", input_shape)\n",
    "        return tuple(input_shape)\n",
    "\n",
    "    def get_batch_size(self):\n",
    "        return self.batch_size\n",
    "\n",
    "    def get_batch(self, names):\n",
    "        if self.current_index + self.batch_size > len(self.data_files):\n",
    "            return None  # 结束校准\n",
    "        \n",
    "        batch_files = self.data_files[self.current_index:self.current_index + self.batch_size]\n",
    "        batch_data = [np.load(f).astype(np.float32) for f in batch_files]\n",
    "        batch_data = np.stack(batch_data, axis=0)  # 确保 batch 维度正确\n",
    "        self.current_index += self.batch_size\n",
    "        \n",
    "        if batch_data.shape != self.input_shape:\n",
    "            raise ValueError(f\"Batch shape {batch_data.shape} does not match expected input shape {self.input_shape}\")\n",
    "        \n",
    "        np.copyto(self.host_input, batch_data)  # 直接复制数据到 Host 端固定页内存\n",
    "        return [ctypes.c_void_p(self.host_input.ctypes.data)]  # 确保返回的是 `void*` 指针\n",
    "    \n",
    "    def read_calibration_cache(self):\n",
    "        cache_file = \"calibration.cache\"\n",
    "        if os.path.exists(cache_file):\n",
    "            with open(cache_file, \"rb\") as f:\n",
    "                return f.read()\n",
    "        return None\n",
    "    \n",
    "    def write_calibration_cache(self, cache):\n",
    "        with open(\"calibration.cache\", \"wb\") as f:\n",
    "            f.write(cache)\n",
    "\n",
    "# 用法示例：使用 TensorRT 进行 INT8 量化转换\n",
    "onnx_model_path = \"/home/guoy/led_detection/training/RetinaNet/RetinaNet_v1.onnx\"\n",
    "calibration_data_folder = \"/home/guoy/led_detection/training/yolov8/yolo_val/extra_npy_fixed\"\n",
    "int8_calibrator = RetinaNetInt8Calibrator(calibration_data_folder, onnx_model_path)\n",
    "\n",
    "logger = trt.Logger(trt.Logger.INFO)\n",
    "builder = trt.Builder(logger)\n",
    "network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\n",
    "parser = trt.OnnxParser(network, logger)\n",
    "\n",
    "with open(onnx_model_path, \"rb\") as model_file:\n",
    "    if not parser.parse(model_file.read()):\n",
    "        for error in range(parser.num_errors):\n",
    "            print(parser.get_error(error))\n",
    "        raise ValueError(\"Failed to parse ONNX model\")\n",
    "\n",
    "config = builder.create_builder_config()\n",
    "config.set_flag(trt.BuilderFlag.INT8)\n",
    "config.int8_calibrator = int8_calibrator\n",
    "\n",
    "serialized_engine = builder.build_serialized_network(network, config)\n",
    "\n",
    "if serialized_engine is None:\n",
    "    raise RuntimeError(\"Failed to build TensorRT engine\")\n",
    "\n",
    "engine_path = \"/home/guoy/led_detection/training/RetinaNet/RetinaNet_v1_int8.engine\"\n",
    "with open(engine_path, \"wb\") as f:\n",
    "    f.write(serialized_engine)\n",
    "\n",
    "print(f\"INT8 TensorRT engine saved at {engine_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尝试2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03/20/2025-23:12:13] [TRT] [I] [MemUsageChange] Init CUDA: CPU -2, GPU +0, now: CPU 40, GPU 1794 (MiB)\n",
      "[03/20/2025-23:12:15] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +2754, GPU +446, now: CPU 2996, GPU 2240 (MiB)\n",
      "ONNX model successfully parsed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_980007/1324079180.py:89: DeprecationWarning: Use Deprecated in TensorRT 10.1. Superseded by explicit quantization. instead.\n",
      "  config.int8_calibrator = Int8EntropyCalibrator2(calibration_data_folder)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Building TensorRT engine, please wait...\n",
      "[03/20/2025-23:12:15] [TRT] [I] Perform graph optimization on calibration graph.\n",
      "[03/20/2025-23:12:15] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[03/20/2025-23:12:15] [TRT] [I] Compiler backend is used during engine build.\n",
      "[03/20/2025-23:12:17] [TRT] [I] Detected 1 inputs and 3 output network tensors.\n",
      "[03/20/2025-23:12:18] [TRT] [I] Total Host Persistent Memory: 313120 bytes\n",
      "[03/20/2025-23:12:18] [TRT] [I] Total Device Persistent Memory: 673280 bytes\n",
      "[03/20/2025-23:12:18] [TRT] [I] Max Scratch Memory: 2151424 bytes\n",
      "[03/20/2025-23:12:18] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 315 steps to complete.\n",
      "[03/20/2025-23:12:18] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 38.375ms to assign 34 blocks to 315 nodes requiring 70206976 bytes.\n",
      "[03/20/2025-23:12:18] [TRT] [I] Total Activation Memory: 70206976 bytes\n",
      "[03/20/2025-23:12:18] [TRT] [I] Total Weights Memory: 215006528 bytes\n",
      "[03/20/2025-23:12:18] [TRT] [I] Compiler backend is used during engine execution.\n",
      "[03/20/2025-23:12:18] [TRT] [I] Engine generation completed in 2.24353 seconds.\n",
      "[03/20/2025-23:12:18] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +68, now: CPU 0, GPU 285 (MiB)\n",
      "🧐 正在获取 batch, 当前索引: 0\n",
      "[03/20/2025-23:12:18] [TRT] [I] Starting Calibration.\n",
      "[03/20/2025-23:12:18] [TRT] [E] [executionContext.cpp::commonEmitDebugTensor::1791] Error Code 2: Internal Error (Assertion attr.type == cudaMemoryTypeHost failed. )\n",
      "❌ Failed to build the TensorRT engine!\n",
      "[03/20/2025-23:12:18] [TRT] [E] [calibrator.cpp::calibrateEngine::1236] Error Code 2: Internal Error (Assertion context->executeV2(bindings.data()) failed. )\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "a bytes-like object is required, not 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 105\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# Save the engine to a file\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(trt_engine_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m--> 105\u001b[0m     \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ TensorRT engine has been saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrt_engine_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: a bytes-like object is required, not 'NoneType'"
     ]
    }
   ],
   "source": [
    "import tensorrt as trt\n",
    "import numpy as np\n",
    "import os\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "\n",
    "onnx_file = \"RetinaNet_v1.onnx\"\n",
    "trt_engine_path = \"RetinaNet_16_int8.trt\"\n",
    "\n",
    "\n",
    "# 🔹 创建 TensorRT Logger\n",
    "logger = trt.Logger(trt.Logger.INFO)\n",
    "\n",
    "# 🔹 创建 TensorRT Builder 和 Network\n",
    "builder = trt.Builder(logger)\n",
    "network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\n",
    "parser = trt.OnnxParser(network, logger)\n",
    "\n",
    "# 🔹 解析 ONNX 模型\n",
    "with open(onnx_file, 'rb') as model:\n",
    "    if not parser.parse(model.read()):\n",
    "        for i in range(parser.num_errors):\n",
    "            print(f\"ONNX Parsing Error {i}: {parser.get_error(i)}\")\n",
    "        raise RuntimeError(\"Failed to parse ONNX model\")\n",
    "    else:\n",
    "        print(\"ONNX model successfully parsed!\")\n",
    "\n",
    "# 🔹 创建 TensorRT 配置\n",
    "config = builder.create_builder_config()\n",
    "config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 8 << 30)  # 8GB 工作空间\n",
    "config.set_flag(trt.BuilderFlag.INT8)  # 开启 INT8 模式\n",
    "\n",
    "# **INT8 校准器**\n",
    "class Int8EntropyCalibrator2(trt.IInt8EntropyCalibrator2):\n",
    "    def __init__(self, calibration_data_path, batch_size=1):\n",
    "        trt.IInt8EntropyCalibrator2.__init__(self)\n",
    "        self.batch_size = batch_size\n",
    "        self.data_path = calibration_data_path\n",
    "        self.data_files = sorted(os.listdir(calibration_data_path))  # 获取所有图片\n",
    "        self.current_index = 0\n",
    "        self.device_input = None\n",
    "        \n",
    "        # 预加载校准数据\n",
    "        self.data = []\n",
    "        for file in self.data_files:\n",
    "            file_path = os.path.join(calibration_data_path, file)\n",
    "            img = np.load(file_path, allow_pickle=False).astype(np.float32)\n",
    "            if img.shape == (640, 640, 3):\n",
    "                img = img.transpose(2, 0, 1)  # 转换为 (3, 640, 640)\n",
    "            img = img.reshape(1, 3, 640, 640)  # 确保是 (1, 3, 640, 640)\n",
    "            img = np.ascontiguousarray(img)\n",
    "\n",
    "            self.data.append(img)\n",
    "        \n",
    "        self.device_input = cuda.mem_alloc(self.batch_size * self.data[0].nbytes)  # 申请显存\n",
    "        \n",
    "    def get_batch_size(self):\n",
    "        return self.batch_size\n",
    "\n",
    "    def get_batch(self, names):\n",
    "        print(f\"🧐 正在获取 batch, 当前索引: {self.current_index}\")\n",
    "        if self.current_index + self.batch_size > len(self.data):\n",
    "            return None  # 结束\n",
    "        batch = self.data[self.current_index:self.current_index + self.batch_size]\n",
    "        \n",
    "        # 先在 CPU 上处理数据\n",
    "        batch = np.ascontiguousarray(batch, dtype=np.float32)  # 确保是 float32\n",
    "        batch = batch.ravel()  # 确保是一维数据\n",
    "        \n",
    "        # **强制拷贝到 CPU 并转换**\n",
    "        batch_cpu = np.empty_like(batch, dtype=np.float32)\n",
    "        np.copyto(batch_cpu, batch)  # 确保数据从 GPU 到 CPU 再传输\n",
    "        \n",
    "        # 拷贝到 GPU\n",
    "        cuda.memcpy_htod(self.device_input, batch_cpu)\n",
    "        self.current_index += self.batch_size\n",
    "        return [self.device_input]\n",
    "\n",
    "    def read_calibration_cache(self):\n",
    "        return None  # 不使用缓存\n",
    "\n",
    "    def write_calibration_cache(self, cache):\n",
    "        pass  # 不写入缓存\n",
    "\n",
    "# Assuming you have a folder with calibration data in .npy format\n",
    "calibration_data_folder = \"/home/guoy/led_detection/training/yolov8/yolo_val/extra_npy\"\n",
    "\n",
    "# 赋值 INT8 校准器\n",
    "config.int8_calibrator = Int8EntropyCalibrator2(calibration_data_folder)\n",
    "\n",
    "# 创建优化配置\n",
    "profile = builder.create_optimization_profile()\n",
    "profile.set_shape(\"input\", (1, 3, 640, 640), (1, 3, 640, 640), (1, 3, 640, 640))  \n",
    "config.add_optimization_profile(profile)\n",
    "\n",
    "# Build the TensorRT engine\n",
    "print(\"🚀 Building TensorRT engine, please wait...\")\n",
    "engine = builder.build_serialized_network(network, config)\n",
    "if engine is None:\n",
    "    print(\"❌ Failed to build the TensorRT engine!\")\n",
    "    exit(1)\n",
    "\n",
    "# Save the engine to a file\n",
    "with open(trt_engine_path, \"wb\") as f:\n",
    "    f.write(engine)\n",
    "print(f\"✅ TensorRT engine has been saved to {trt_engine_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "个人觉得最接近正确的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'trt' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n trt ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import tensorrt as trt\n",
    "import numpy as np\n",
    "import os\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "import ctypes\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "onnx_file = \"./RetinaNet_v3.onnx\"\n",
    "engine_file = \"./model_8.trt\"\n",
    "calibration_data_path = \"./cali_datei\"\n",
    "\n",
    "# ✅ TensorRT Logger\n",
    "logger = trt.Logger(trt.Logger.INFO)\n",
    "builder = trt.Builder(logger)\n",
    "network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\n",
    "parser = trt.OnnxParser(network, logger)\n",
    "\n",
    "# ✅ Load ONNX Model\n",
    "with open(onnx_file, \"rb\") as model:\n",
    "    if not parser.parse(model.read()):\n",
    "        for i in range(parser.num_errors):\n",
    "            print(f\"ONNX Parsing Error {i}: {parser.get_error(i)}\")\n",
    "        raise RuntimeError(\"Failed to parse ONNX model\")\n",
    "    else:\n",
    "        print(\"✅ ONNX model successfully parsed!\")\n",
    "\n",
    "# ✅ Create TensorRT Config\n",
    "config = builder.create_builder_config()\n",
    "config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 8 << 30)\n",
    "config.set_flag(trt.BuilderFlag.INT8)  # 🔥 Enable INT8\n",
    "config.set_flag(trt.BuilderFlag.DISABLE_TIMING_CACHE)  # 🔥 Avoid Timing Cache Errors\n",
    "\n",
    "import tensorrt as trt\n",
    "import numpy as np\n",
    "import os\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "import ctypes\n",
    "\n",
    "class Int8Calibrator(trt.IInt8EntropyCalibrator2):\n",
    "    def __init__(self, calibration_data_folder, batch_size=1, cache_file=\"calibration.cache\"):\n",
    "        super(Int8Calibrator, self).__init__()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.cache_file = cache_file\n",
    "\n",
    "        # 🔥 获取所有 .npy 文件\n",
    "        self.image_paths = sorted([\n",
    "            os.path.join(calibration_data_folder, f)\n",
    "            for f in os.listdir(calibration_data_folder) if f.endswith(\".npy\")\n",
    "        ])\n",
    "        self.current_index = 0\n",
    "\n",
    "        # ✅ 预分配 GPU 设备内存 (Device Memory)\n",
    "        self.device_input = cuda.mem_alloc(batch_size * 3 * 640 * 640 * np.float32().nbytes)\n",
    "\n",
    "        # ✅ 预分配 CPU (Host) 内存\n",
    "        self.pinned_memory = np.zeros((batch_size, 3, 640, 640), dtype=np.float32)\n",
    "\n",
    "        # ✅ 创建一个生成器 (batches) 提供数据\n",
    "        self.batches = self._batch_generator()\n",
    "\n",
    "        print(f\"✅ Initialized Int8Calibrator with {len(self.image_paths)} calibration images\")\n",
    "\n",
    "    def _batch_generator(self):\n",
    "        \"\"\" 生成校准数据 \"\"\"\n",
    "        for i in range(0, len(self.image_paths), self.batch_size):\n",
    "            batch = []\n",
    "            for j in range(self.batch_size):\n",
    "                if i + j < len(self.image_paths):\n",
    "                    npy_file = self.image_paths[i + j]\n",
    "                    img = np.load(npy_file).astype(np.float32)\n",
    "\n",
    "                if img.shape == (1, 3, 640, 640):\n",
    "                    img = img.squeeze(0)\n",
    "\n",
    "                    batch.append(img)\n",
    "\n",
    "            if len(batch) > 0:\n",
    "                yield np.array(batch)\n",
    "\n",
    "    def get_batch_size(self):\n",
    "        return self.batch_size\n",
    "\n",
    "    def get_batch(self, names):\n",
    "        try:\n",
    "            # ✅ 从生成器获取数据\n",
    "            data = next(self.batches)\n",
    "\n",
    "            print(f\"✅ Loaded batch, shape: {data.shape}\")\n",
    "\n",
    "            # ✅ 复制数据到 GPU\n",
    "            cuda.memcpy_htod(self.device_input, data)\n",
    "\n",
    "            # ✅ 返回 GPU 设备内存指针\n",
    "            return [int(self.device_input)]\n",
    "\n",
    "        except StopIteration:\n",
    "            print(\"❌ No more calibration data available\")\n",
    "            return None\n",
    "\n",
    "    def read_calibration_cache(self):\n",
    "        \"\"\"✅ 读取 INT8 校准缓存\"\"\"\n",
    "        try:\n",
    "            with open(self.cache_file, \"rb\") as f:\n",
    "                cache = f.read()\n",
    "            print(f\"✅ Using existing INT8 calibration cache: {self.cache_file}\")\n",
    "            return cache\n",
    "        except FileNotFoundError:\n",
    "            print(f\"❌ Calibration cache not found, running fresh calibration\")\n",
    "            return None\n",
    "\n",
    "    def write_calibration_cache(self, cache):\n",
    "        \"\"\"✅ 写入 INT8 校准缓存\"\"\"\n",
    "        if cache is None or len(cache) == 0:\n",
    "            print(\"❌ Calibration cache is empty, possible issue!\")\n",
    "        else:\n",
    "            with open(self.cache_file, \"wb\") as f:\n",
    "                f.write(cache)\n",
    "            print(f\"✅ Calibration cache saved: {self.cache_file}\")\n",
    "\n",
    "# ✅ Apply INT8 Calibrator\n",
    "config.int8_calibrator = Int8Calibrator(calibration_data_path)\n",
    "\n",
    "# ✅ Create Optimization Profile\n",
    "profile = builder.create_optimization_profile()\n",
    "profile.set_shape(\"input\", (1, 3, 640, 640), (1, 3, 640, 640), (1, 3, 640, 640))  # 🔥 Static batch size = 1\n",
    "config.add_optimization_profile(profile)\n",
    "\n",
    "# ✅ Build TensorRT INT8 Engine\n",
    "serialized_engine = builder.build_serialized_network(network, config)\n",
    "\n",
    "if serialized_engine is None:\n",
    "    raise RuntimeError(\"❌ Failed to build TensorRT INT8 engine!\")\n",
    "\n",
    "# ✅ Save TensorRT INT8 Engine\n",
    "with open(engine_file, \"wb\") as f:\n",
    "    f.write(serialized_engine)\n",
    "\n",
    "print(f\"✅ INT8 TensorRT engine saved at: {engine_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
